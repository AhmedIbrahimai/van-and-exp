{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsLdHFH4e6_4"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.initializers import RandomNormal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train , y_train) , (x_test,y_test) = mnist.load_data()\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4SttUPSfjuP",
        "outputId": "76ce76db-e623-4ec8-ddbd-4ca045fe2dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],28*28)\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e0u8WTVfjzY",
        "outputId": "3d9a040c-23d3-465e-dcd0-db89c5d3d261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train / 255.0\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vtWh8nmfj3R",
        "outputId": "ab33dab9-d628-41f6-b8de-9cc442a360ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_init = RandomNormal(mean=1,stddev=1)"
      ],
      "metadata": {
        "id": "PFImYBh4goH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation= 'sigmoid', kernel_initializer =w_init , input_dim=28*28))\n",
        "model.add(Dense(128, activation= 'sigmoid', kernel_initializer =w_init ))\n",
        "model.add(Dense(10, activation= 'softmax', kernel_initializer =w_init))\n",
        "model.compile(loss = 'sparse_categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3hpCZFMgoLD",
        "outputId": "d6b0878a-a44c-4553-cc8e-3c14d395bdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train,y_train,epochs=10 , batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-pWIH-GiWMN",
        "outputId": "4fb10967-70df-4371-8ef8-0f0ce91c4bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 4.0310 - accuracy: 0.1079\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 2.3059 - accuracy: 0.1059\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.3069 - accuracy: 0.1030\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 2.3077 - accuracy: 0.1043\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 2.3077 - accuracy: 0.1036\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.3080 - accuracy: 0.1034\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.3077 - accuracy: 0.1032\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 2.3091 - accuracy: 0.1056\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 2.3099 - accuracy: 0.1027\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.3081 - accuracy: 0.1073\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7d2a920c7490>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xv2pLbSv9UnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-NP1jU3k9UrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 784) / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI5dtZU29UvA",
        "outputId": "c15de2f0-027d-443d-b08c-b2dc4292315d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFP-YUEc9bjU",
        "outputId": "23b16113-21d8-4777-a585-7eeacad69326"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFHMFsKMAGxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIniR_slAG0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 784) / 255.0\n",
        "\n",
        "# Create a simple neural network with two layers\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(256, input_shape=(784,), activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Perform gradient explosion\n",
        "for _ in range(100):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(X_train)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_train, predictions)) #reduce_mean compute the average loss across all the training samples\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    gradients = [tf.clip_by_value(grad, -1000, 1000) for grad in gradients]  # Gradient clipping\n",
        "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Print the gradient norms\n",
        "    gradient_norms = [tf.norm(grad).numpy() for grad in gradients]\n",
        "    print(\"Gradient Norms:\", gradient_norms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nvrSbiB9UyB",
        "outputId": "f2158dc7-267f-4a15-898c-a7e7f77f075e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Norms: [0.40554363, 0.037633464, 0.9383049, 0.11350581]\n",
            "Gradient Norms: [0.39817008, 0.034831524, 0.8740532, 0.105424084]\n",
            "Gradient Norms: [0.39198062, 0.032326926, 0.81643146, 0.09815311]\n",
            "Gradient Norms: [0.3867349, 0.030071827, 0.7643879, 0.09156288]\n",
            "Gradient Norms: [0.38224941, 0.028028265, 0.7170914, 0.08555075]\n",
            "Gradient Norms: [0.37838262, 0.026165677, 0.67387956, 0.080034785]\n",
            "Gradient Norms: [0.37502465, 0.024459409, 0.6342207, 0.07494923]\n",
            "Gradient Norms: [0.37208933, 0.022889437, 0.59768486, 0.070240505]\n",
            "Gradient Norms: [0.36950853, 0.021439299, 0.5639222, 0.0658647]\n",
            "Gradient Norms: [0.36722788, 0.02009555, 0.53264624, 0.061785966]\n",
            "Gradient Norms: [0.36520353, 0.018846983, 0.5036205, 0.057974607]\n",
            "Gradient Norms: [0.36339974, 0.01768428, 0.47664863, 0.054405626]\n",
            "Gradient Norms: [0.36178723, 0.016599627, 0.45156598, 0.05105845]\n",
            "Gradient Norms: [0.36034146, 0.015586431, 0.42823318, 0.04791513]\n",
            "Gradient Norms: [0.35904196, 0.0146390945, 0.40653098, 0.04496066]\n",
            "Gradient Norms: [0.35787132, 0.0137527445, 0.386356, 0.042181663]\n",
            "Gradient Norms: [0.35681462, 0.012923244, 0.36761746, 0.039566826]\n",
            "Gradient Norms: [0.3558589, 0.012146905, 0.35023418, 0.037106194]\n",
            "Gradient Norms: [0.3549929, 0.011420523, 0.33413258, 0.03479049]\n",
            "Gradient Norms: [0.3542068, 0.01074121, 0.3192448, 0.032611605]\n",
            "Gradient Norms: [0.35349187, 0.010106345, 0.30550727, 0.030562153]\n",
            "Gradient Norms: [0.35284033, 0.009513609, 0.29285952, 0.028635208]\n",
            "Gradient Norms: [0.35224536, 0.008960794, 0.28124338, 0.026824588]\n",
            "Gradient Norms: [0.35170072, 0.0084458925, 0.27060223, 0.025124041]\n",
            "Gradient Norms: [0.351201, 0.007967034, 0.26088062, 0.023528242]\n",
            "Gradient Norms: [0.3507412, 0.0075224256, 0.25202394, 0.022031771]\n",
            "Gradient Norms: [0.350317, 0.0071103903, 0.24397832, 0.020629646]\n",
            "Gradient Norms: [0.34992436, 0.0067293197, 0.23669063, 0.0193171]\n",
            "Gradient Norms: [0.34955975, 0.00637767, 0.2301085, 0.018089563]\n",
            "Gradient Norms: [0.34922004, 0.006053942, 0.22418062, 0.016942613]\n",
            "Gradient Norms: [0.34890231, 0.0057566757, 0.21885683, 0.0158722]\n",
            "Gradient Norms: [0.3486041, 0.005484447, 0.21408844, 0.014874351]\n",
            "Gradient Norms: [0.34832314, 0.005235869, 0.20982856, 0.013945215]\n",
            "Gradient Norms: [0.3480574, 0.005009568, 0.20603226, 0.013081176]\n",
            "Gradient Norms: [0.34780505, 0.004804196, 0.20265685, 0.012278771]\n",
            "Gradient Norms: [0.34756455, 0.004618422, 0.19966204, 0.011534677]\n",
            "Gradient Norms: [0.34733447, 0.0044509354, 0.19701007, 0.010845725]\n",
            "Gradient Norms: [0.34711355, 0.0043004462, 0.19466574, 0.010208889]\n",
            "Gradient Norms: [0.3469007, 0.0041656937, 0.19259664, 0.009621248]\n",
            "Gradient Norms: [0.34669492, 0.0040454464, 0.19077292, 0.009080039]\n",
            "Gradient Norms: [0.34649536, 0.0039385147, 0.18916735, 0.008582592]\n",
            "Gradient Norms: [0.3463013, 0.0038437438, 0.18775514, 0.008126341]\n",
            "Gradient Norms: [0.34611195, 0.003760027, 0.18651393, 0.0077087935]\n",
            "Gradient Norms: [0.34592685, 0.0036863266, 0.18542361, 0.00732761]\n",
            "Gradient Norms: [0.34574544, 0.003621646, 0.18446615, 0.0069804555]\n",
            "Gradient Norms: [0.34556723, 0.0035650723, 0.18362546, 0.0066651152]\n",
            "Gradient Norms: [0.3453919, 0.0035157364, 0.18288724, 0.006379428]\n",
            "Gradient Norms: [0.34521902, 0.0034728518, 0.18223882, 0.0061212997]\n",
            "Gradient Norms: [0.34504834, 0.0034356902, 0.18166895, 0.005888735]\n",
            "Gradient Norms: [0.34487957, 0.0034035807, 0.18116775, 0.005679737]\n",
            "Gradient Norms: [0.34471253, 0.0033759382, 0.18072654, 0.0054924227]\n",
            "Gradient Norms: [0.3445469, 0.003352221, 0.18033768, 0.0053249784]\n",
            "Gradient Norms: [0.34438264, 0.003331949, 0.17999446, 0.005175685]\n",
            "Gradient Norms: [0.3442195, 0.0033146876, 0.17969105, 0.005042873]\n",
            "Gradient Norms: [0.3440574, 0.0033000563, 0.17942229, 0.0049249865]\n",
            "Gradient Norms: [0.34389624, 0.0032877163, 0.17918372, 0.004820552]\n",
            "Gradient Norms: [0.34373584, 0.0032773921, 0.17897145, 0.0047281818]\n",
            "Gradient Norms: [0.34357616, 0.0032687923, 0.17878214, 0.004646608]\n",
            "Gradient Norms: [0.34341714, 0.003261706, 0.17861277, 0.0045746695]\n",
            "Gradient Norms: [0.3432587, 0.0032559375, 0.1784608, 0.0045112777]\n",
            "Gradient Norms: [0.34310076, 0.0032513032, 0.17832401, 0.0044554486]\n",
            "Gradient Norms: [0.34294328, 0.0032476555, 0.17820045, 0.0044063027]\n",
            "Gradient Norms: [0.34278625, 0.003244865, 0.17808847, 0.0043630116]\n",
            "Gradient Norms: [0.3426296, 0.003242812, 0.17798658, 0.004324907]\n",
            "Gradient Norms: [0.3424733, 0.0032414112, 0.17789355, 0.0042913123]\n",
            "Gradient Norms: [0.34231734, 0.0032405583, 0.17780824, 0.0042616557]\n",
            "Gradient Norms: [0.3421617, 0.0032401937, 0.17772973, 0.004235448]\n",
            "Gradient Norms: [0.34200633, 0.0032402507, 0.1776572, 0.0042122263]\n",
            "Gradient Norms: [0.34185123, 0.003240676, 0.17758995, 0.0041916226]\n",
            "Gradient Norms: [0.34169638, 0.0032414168, 0.17752735, 0.004173286]\n",
            "Gradient Norms: [0.34154174, 0.0032424366, 0.17746887, 0.004156908]\n",
            "Gradient Norms: [0.34138736, 0.0032437097, 0.17741401, 0.004142234]\n",
            "Gradient Norms: [0.34123316, 0.003245187, 0.17736243, 0.0041290354]\n",
            "Gradient Norms: [0.3410792, 0.0032468543, 0.17731375, 0.004117104]\n",
            "Gradient Norms: [0.34092543, 0.0032486885, 0.17726767, 0.004106269]\n",
            "Gradient Norms: [0.34077185, 0.0032506718, 0.17722394, 0.004096397]\n",
            "Gradient Norms: [0.3406185, 0.0032527808, 0.17718232, 0.0040873457]\n",
            "Gradient Norms: [0.34046528, 0.003255019, 0.17714263, 0.0040789894]\n",
            "Gradient Norms: [0.34031227, 0.0032573529, 0.17710468, 0.004071273]\n",
            "Gradient Norms: [0.34015942, 0.0032597815, 0.17706834, 0.004064053]\n",
            "Gradient Norms: [0.34000674, 0.0032623054, 0.17703347, 0.0040572854]\n",
            "Gradient Norms: [0.33985424, 0.003264898, 0.17699997, 0.0040509314]\n",
            "Gradient Norms: [0.3397019, 0.0032675674, 0.17696771, 0.004044899]\n",
            "Gradient Norms: [0.33954972, 0.0032702987, 0.17693667, 0.0040391805]\n",
            "Gradient Norms: [0.33939773, 0.003273102, 0.17690672, 0.004033683]\n",
            "Gradient Norms: [0.3392459, 0.0032759595, 0.1768778, 0.0040284097]\n",
            "Gradient Norms: [0.3390942, 0.0032788652, 0.17684986, 0.0040233405]\n",
            "Gradient Norms: [0.33894268, 0.0032818315, 0.17682286, 0.0040184157]\n",
            "Gradient Norms: [0.3387913, 0.0032848408, 0.17679678, 0.004013643]\n",
            "Gradient Norms: [0.33864012, 0.0032878944, 0.17677152, 0.0040089693]\n",
            "Gradient Norms: [0.33848906, 0.0032909994, 0.17674708, 0.0040044175]\n",
            "Gradient Norms: [0.33833817, 0.0032941524, 0.17672345, 0.00399993]\n",
            "Gradient Norms: [0.33818743, 0.003297343, 0.17670056, 0.0039955312]\n",
            "Gradient Norms: [0.33803686, 0.003300571, 0.17667842, 0.003991209]\n",
            "Gradient Norms: [0.33788642, 0.0033038403, 0.176657, 0.003986926]\n",
            "Gradient Norms: [0.33773613, 0.0033071463, 0.17663628, 0.003982707]\n",
            "Gradient Norms: [0.337586, 0.0033104909, 0.17661624, 0.003978521]\n",
            "Gradient Norms: [0.33743602, 0.003313869, 0.17659687, 0.0039743716]\n",
            "Gradient Norms: [0.3372862, 0.0033172888, 0.17657813, 0.003970267]\n",
            "Gradient Norms: [0.3371365, 0.003320742, 0.17656006, 0.00396617]\n"
          ]
        }
      ]
    }
  ]
}